[TOC]
# 1 存储场景的特点
1. 追加写
存量数据不涉及修改和更新
2. 写多读少
写：节点会不断产生统计信息，会不断追加数据
读：只存在外部请求到来时
3. 数据是KV型的
产生的数据为key:value类型 rtt:1000 cpu:92.12
4. 需要持久化
由于产生的数据很多，所以不可能都在内存中，且数据需要有恢复手段

# 2 存储位置技术选型
目前可选的方案有：
1. 本地缓存：本地缓存，或者使用redis
2. 本地disk：本地数据库
3. 远程mysql clickhouse
4. 远程COS

## 2.1 先讨论远程存储 

如果是**远程中心式的数据库mysql，clickhouse(或数据库集群)的话**，其实与之前的方案是一样的，也会出现现有的瓶颈

**远程COS**，存在的问题：
1. COS并不是从服务器实时上传的，而是将一段时间（比如一天或1小时）产生的数据文件上传，这样就会有问题——如果查询当前的用户信息就会在COS中查不到，还需要到对应的节点内存里面找
2. 请求机器比如http网关从COS下载文件load到内存中，这个过程是http协议，有大文件传输的问题，传输时间是个问题，
3. http网关获取到文件后需要在内存中检索相关信息，耗费CPU和内存，而这里的网关也是单点的，瓶颈会很严重，不符合当初的设计思路

## 2.2 本地缓存？
问题：完全放在本地缓存，不持久化的话，数据量太大，内存成本高，且数据不安全
使用redis也是一样的，redis的持久化对象是数据操作log，用于恢复数据，数据本身还是全部保存在内存中的，而由于当前场景的数据不可能全部放入内存，所以需要选用持久化的kv数据库
## 2.3 **使用本地disk：基于持久化数据库**
一个方案是使用sqlite，是一个轻型的关系型数据库
一个方案是使用levelDB，是一个持久化的KV型数据库
还有一个方案是用自研的数据库引擎，可以仿照prometheus的自研tsdb

### 2.3.1 sqlit数据库调研笔记及选型分析
（嵌入式）本地数据库 小 写入慢 多用于读多写少
服务器进程 不支持分布式 
表级别锁 写的时候不能查 查的时候不能写
sqlite的场景，是否一定要用到sqlite的数据库功能，如果仅仅是查询数据，不删除修改数据，完全没有必要引入sqlite。
sqlit基本操作C++(https://www.cnblogs.com/hankkk/p/5782321.html)
**选型分析：**
1、这里的查和写针对的是不同的数据条目，应该不存在公共区，也就是说读和写应该是互不干扰的
2、对于外部请求来说就只是数据查询，没有数据修改和删除，所以是不是没有必要使用sqlite的数据库功能，
3、sqlite数据库等关系型数据库创建索引之后，写入速度会很慢，而当前场景主要是写场景，日志会源源不断写入磁盘文件。日志文件本身就是KV数据，所以没必要用关系型数据库

### 2.3.2 leveldb调研笔记：
LevelDB是Google开源的持久化KV单机数据库，具有很高的随机写，顺序读/写性能，但是随机读的性能很一般，也就是说，LevelDB很适合应用在查询较少，而写很多的场景。
特点：
1、key和value都是任意长度的字节数组；
2、entry（即一条K-V记录）默认是按照key的字典顺序存储的，当然开发者也可以重载这个排序函数；
3、提供的基本操作接口：Put()、Delete()、Get()、Batch()；
4、支持批量操作以原子操作进行；
5、可以创建数据全景的snapshot(快照)，并允许在快照中查找数据；
6、可以通过前向（或后向）迭代器遍历数据（迭代器会隐含的创建一个snapshot）；
7、自动使用Snappy压缩数据；
8、可移植性；
限制：
1、非关系型数据模型（NoSQL），不支持sql语句；
2、一次只允许一个进程访问一个特定的数据库；
leveldb相较于redis来说，他是持久化的，
levelDB存储方案：
两级缓存(memtable imm)+SSTable（存在硬盘上 内容按key排序）
写方案：
顺序写->wal(记录写操作，用于节点挂掉之后内存中的数据恢复)->memtable(缓存 SkipList，对键排序)->dump SStable上（依旧是追加写）
![leveldb存储结构图.jpg](/tencent/api/attachments/s3/url?attachmentid=269669)
level数据存储使用LSM 日志压缩树
而压缩意味着在日志中丢弃重复的键，只保留每一个键的最近更新
**这在本场景似乎是不能接受的**
查方案：
先在内存中的memtable（排序树）上查，（有序树查找速度快 + 当前内存中的数据是热数据（），查到的概率高）
而后在sstable逐级查找，查找方法使用了LSM（log-structured Merge Tree 优势是写入速度）原理
使用FileMetaData储存所有sstable的文件信息，记录了文件号和key值的范围，可以用文件号快速定位到磁盘上的一个sstable文件，接着比较该sstable的key值范围是否符合要求，如果不符合就就一直向下一级查询，直到查到最后一级（为了防止查询一个不存在的键的时间开销，可以使用布隆过滤器）
![levelDB查询过程.jpg](/tencent/api/attachments/s3/url?attachmentid=269670)
### 2.3.3 prometheus调研笔记：
**tsdb:**
1. 将数据存储为time-value格式的kv型数据库，跟其他数据库相比，时间是一个必不可少的维度
2. 时序数据库的应用场景：时效性；写多读少；高吞吐写入
3. 比较出名的是influxDB和Prometheus
influxDB 底层原理基于levelDB，后续版本演化为自研的TSM存储引擎
Prometheus 底层v3自研存储引擎

**prometheus time series**
![](./prometheus相关名词解释.png)
相关的名词解释
series:label-value
timestamp-value

**V2存储引擎**
V2的磁盘分区：
将每一个series作为一个文件，series文件内按照timestamp有序，即上图中每一条横线为一个文件
写坏处：
**每个时间点下，会有多个series需要写入，这样需要打开多个series，且每次只写入少量数据**
-> V2使用了chunk写解决这个问题，需要数据在内存中堆积到一定的时间线后才写入，不仅解决了上面的问题，同时热数据在内存中，提高了查找数据
-> 但即使使用了chunk，如果一次写入涉及很多时间线（多个series）,IOPS要求会很高
读坏处：
**如果是复合条件查询涉及多个series 需要打开很多文件 一次查询需要打开大量的文件**
另外：
**过期数据回收需要从每个文件中扫描和淘汰**

**V3存储引擎**
V3引擎可以看做是一个简单版、针对时序数据场景优化过后的LSM
V3磁盘分区：
将数据按照时间维度分为多个block，每个block之间相互独立，数据没有交叉，即上图中的一个矩形范围为一个block，block下同样有chunks，作用与V2是一样的，避免每次写入小数据。只是这样的话各个series就不是隔离的，同一个chunk下有多个series，需要做好索引，所以每个block下有index文件，用来对chunk中的series快速查找,可以支持根据某个label快速定位到时间线以及数据所在的chunk。
V3版本的目录结构如图所示：
```
./data
├── 01BKGV7JBM69T2G1BGBGM6KB12
│   └── meta.json
├── 01BKGTZQ1SYQJTR4PB43C8PD98
│   ├── chunks
│   │   └── 000001
│   ├── tombstones
│   ├── index
│   └── meta.json
├── 01BKGTZQ1HHWHV8FBJXW1Y3W0K
│   └── meta.json
├── 01BKGV7JC0RY8A6MACW02A2PJD
│   ├── chunks
│   │   └── 000001
│   ├── tombstones
│   ├── index
│   └── meta.json
├── chunks_head
│   └── 000001
└── wal
    ├── 000000002
    └── checkpoint.00000001
        └── 00000000
```
各个文件的数据保存格式可以参考  prometheus/tsdb/docs/format/
Index
Chunks
Head Chunks
Tombstones
Wal
Memory Snapshot
也可以参考下图：
![prometheus各个文件数据保存格式.jpg](/tencent/api/attachments/s3/url?attachmentid=269672)
V3内存分区：
类似于LSM的两级缓存结构
内存中维护一个排序树或者跳表用来对错序的时间key排序
immtable->chunk结构

写方案：完全类似于LSM 可以参考上面levelDB的实现
不同的是上面的sstable是block 一般两个小时为维度
读方案：类似于LSM的方案 但是index方案不太一样
每一个已经落盘的block，其index已经固定了，不可修改

优点：
顺序写：内存dump到磁盘的是大块数据block
读：当查询维度有时间时，可以快速排除无关的block
淘汰老数据很方便

**TODO:V2和V3的索引方案还需了解更具体**
**TODO:V3使用了LSM一样的compaction策略来做查询优化 LSM**
数据压缩的疑问：压缩的目的在于减少磁盘空间 加快查找速度，但压缩会不会丢掉一部分历史数据

### 2.3.4  **选型分析**
选择时序数据库
目前常见的时序数据库有以下几种：
使用类LSM的存储引擎还是有不少优势的：
有些TSDB直接基于开源的LSM引擎分布式数据库例如Hbase或Cassandra实现底层存储
也有自己基于LevelDB/RocksDB研发，
像InfluxDB和**Prometheus的tsdb一样纯自研**，因为时序数据这一特定场景还是可以做更多的优化，例如索引、compaction策略等。
现成的时序数据库组件例如influxDB和promethus的tsdb，一般是作为中心数据存储数据库，汇聚多个节点的数据，作为每个节点本地数据库来说的话不太合适，这里只需要他们的时序数据库本身的数据存取能力。
于是得到了结论：**实现一个类似influxDB和promethusV3的本地数据存储引擎**，更加注重轻量化和定制化。

时序数据库方案的缺点：
1. 每个serise按照时间排序
按照时间默认有序（对于无顺序数据点——网络延时或者时钟问题 需要重排）
这样的话按照其他维度查询时，势必会引入复杂查询语句，而且需要另外建立维度相关的索引